{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setAppName(\"pyspark\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "#btc = sc.parallelize([(0,1), (1,2), (2,5), (5,8), (7,8), (3,7), (3,4), (3,6), (10,11), (10,12), (12,13)])\n",
    "#btc = btc_raw.flatMap(lambda x: [x, (x[1], x[0])]).groupByKey().map(lambda x: (x[0], set(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency_list = sc.parallelize([(0,1), (1,2), (2,5), (5,8), (7,8), (3,7), (3,4), (3,6), (10,11), (10,12), (12,13)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_edges(line):\n",
    "    a = [x for x in line]\n",
    "    #a = [int(x) for x in line.split(\",\")]\n",
    "    edges_list=[]\n",
    "    edges_list.append((a[0],a[1]))\n",
    "    edges_list.append((a[1],a[0]))\n",
    "    return edges_list\n",
    "\n",
    "# adj_list.txt is a txt file containing adjacency list of the graph.\n",
    "#adjacency_list = sc.textFile(\"graph3.txt\")\n",
    "\n",
    "edges_rdd = adjacency_list.flatMap(lambda line : create_edges(line)).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 7),\n",
       " (7, 3),\n",
       " (10, 12),\n",
       " (12, 10),\n",
       " (0, 1),\n",
       " (1, 2),\n",
       " (5, 2),\n",
       " (8, 5),\n",
       " (7, 8),\n",
       " (3, 4),\n",
       " (6, 3),\n",
       " (10, 11),\n",
       " (12, 13),\n",
       " (1, 0),\n",
       " (2, 1),\n",
       " (2, 5),\n",
       " (5, 8),\n",
       " (8, 7),\n",
       " (4, 3),\n",
       " (3, 6),\n",
       " (11, 10),\n",
       " (13, 12)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0\n",
      "iter 1\n"
     ]
    }
   ],
   "source": [
    "def largeStarInit(record):\n",
    "    a, b = record\n",
    "    yield (a,b)\n",
    "    yield (b,a)\n",
    "\n",
    "def largeStar(record):\n",
    "    a, b = record\n",
    "    t_list = list(b)\n",
    "    t_list.append(a)\n",
    "    list_min = min(t_list)\n",
    "    for x in b:\n",
    "        if a < x:\n",
    "            yield (x,list_min)\n",
    "\n",
    "def smallStarInit(record):\n",
    "    a, b = record\n",
    "    if b<=a:\n",
    "        yield (a,b)\n",
    "    else:\n",
    "        yield (b,a)\n",
    "\n",
    "def smallStar(record):\n",
    "    a, b = record\n",
    "    t_list = list(b)\n",
    "    t_list.append(a)\n",
    "    list_min = min(t_list)\n",
    "    for x in t_list:\n",
    "        if x!=list_min:\n",
    "            yield (x,list_min)\n",
    "\n",
    "#Handle case for single nodes\n",
    "def single_vertex(line):\n",
    "    a = [x for x in line]\n",
    "    #a = [int(x) for x in line.split(\",\")]\n",
    "    edges_list=[]\n",
    "    if len(a)==1:\n",
    "        edges_list.append((a[0],a[0]))\n",
    "    return edges_list\n",
    "\n",
    "iteration_num =0 \n",
    "while 1==1:\n",
    "    if iteration_num==0:\n",
    "        print(\"iter\", iteration_num)\n",
    "        large_star_rdd = edges_rdd.groupByKey().flatMap(lambda x : largeStar(x))\n",
    "        small_star_rdd = large_star_rdd.flatMap(lambda x : smallStarInit(x)).groupByKey().flatMap(lambda x : smallStar(x)).distinct()\n",
    "        iteration_num += 1\n",
    "        \n",
    "    else:\n",
    "        print(\"iter\", iteration_num)\n",
    "        large_star_rdd = small_star_rdd.flatMap(lambda x: largeStarInit(x)).groupByKey().flatMap(lambda x : largeStar(x)).distinct()\n",
    "        small_star_rdd = large_star_rdd.flatMap(lambda x : smallStarInit(x)).groupByKey().flatMap(lambda x : smallStar(x)).distinct()\n",
    "        iteration_num += 1\n",
    "    #check Convergence\n",
    "\n",
    "    changes = (large_star_rdd.subtract(small_star_rdd).union(small_star_rdd.subtract(large_star_rdd))).collect()\n",
    "    if len(changes) == 0 :\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_vertex_rdd = adjacency_list.flatMap(lambda line : single_vertex(line)).distinct()\n",
    "single_vertex_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 0), (12, 10), (4, 2), (3, 0), (13, 10), (8, 0), (6, 2), (1, 0), (5, 0), (11, 10), (7, 2)]\n",
      "[(0, [2, 3, 8, 1, 5]), (10, [12, 13, 11]), (2, [4, 6, 7])]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "answer = single_vertex_rdd.collect() + large_star_rdd.collect()\n",
    "\n",
    "sol = large_star_rdd.map(lambda x: (x[1] , x[0])).groupByKey().mapValues(list).collect()\n",
    "\n",
    "print(answer)\n",
    "print(sol)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
