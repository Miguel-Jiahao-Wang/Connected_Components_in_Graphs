{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "conf = SparkConf().setAppName(\"pyspark\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Node: integer (nullable = false)\n",
      " |-- NN: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c615e6236351>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;31m#Cracker with findSeeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m \u001b[0mTree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcracker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSeeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0mseed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "#Graph: (node, [NN])\n",
    "schemaList = [\"Node\", \"NN\"]\n",
    "schemaType = [ IntegerType() , ArrayType( IntegerType() )]\n",
    "schemaNull = [False, True]\n",
    "\n",
    "fields = [StructField(schemaList[0], schemaType[0], schemaNull[0]),\\\n",
    "          StructField(schemaList[1], schemaType[1], schemaNull[1])]\n",
    "\n",
    "schemaG = StructType(fields)\n",
    "\n",
    "#Tree with (Parent, Child) format\n",
    "schemaList2 = [\"Parent\", \"Child\"]\n",
    "schemaType2 = [ IntegerType() , IntegerType() ]\n",
    "schemaNull2 = [False, True]\n",
    "\n",
    "fields2 = [StructField(schemaList2[0], schemaType2[0], schemaNull2[0]),\\\n",
    "          StructField(schemaList2[1], schemaType2[1], schemaNull2[1])]\n",
    "\n",
    "schemaT = StructType(fields2)\n",
    "\n",
    "#Seeds\n",
    "schemaList3 = [\"Node\"]\n",
    "schemaType3 = [IntegerType()]\n",
    "schemaNull3 = [True]\n",
    "\n",
    "fields3 = [StructField(schemaList3[0], schemaType3[0], schemaNull3[0])]\n",
    "\n",
    "schemaS = StructType(fields3)\n",
    "\n",
    "\n",
    "\n",
    "#-------------Min_Selection_Step--------------------------\n",
    "def Min_Selection_Step(df_G):\n",
    "    #Including node inside NN_H and finding the min_id inside the NN_H\n",
    "    NN_min = df_G.select(array_union(col(\"NN\"), array(col(\"Node\"))).alias(\"NN\")\\\n",
    "                        ).withColumn(\"v_min\", array_min( col(\"NN\") ))\n",
    "    #All edges that needs to be added to G_{t+1}\n",
    "    addEdge = NN_min.select(explode(NN_min.NN).alias(\"Node\"), \"v_min\")\n",
    "    #Grouping all node_id\n",
    "    dfH = addEdge.groupBy(\"Node\").agg(collect_set(\"v_min\").alias(\"NN\"))\n",
    "    return dfH\n",
    "\n",
    "#-------------Pruning_Step--------------------------\n",
    "#----------Useful functions\n",
    "#Function to check if int is inside an array\n",
    "#Used in deactiveNodes\n",
    "def isInside(node, NN):\n",
    "    return (node in NN)\n",
    "\n",
    "#Turning the function into Pyspark User Defined Function\n",
    "isInside_udf = udf(isInside, BooleanType())\n",
    "\n",
    "#Function to join two list\n",
    "#Used to find seeds\n",
    "def joinList(H_NN, G_NN):\n",
    "    #I need to think if H_NN can be empty\n",
    "    if G_NN != None:\n",
    "        return list(set(G_NN + H_NN))\n",
    "    else:\n",
    "        return H_NN\n",
    "\n",
    "#Turning the function into Pyspark User Defined Function\n",
    "joinList_udf = udf(joinList, ArrayType(IntegerType()))\n",
    "\n",
    "#------------Main Pruning part\n",
    "def Pruning_Step(dfH, T, Seeds):\n",
    "\n",
    "    #---------------G construction-------------------\n",
    "    H_filtered = dfH.filter(size(col(\"NN\")) > 1) #NN with more than 1 element\n",
    "    NN_H_min = H_filtered.select(\"NN\", array_min(col(\"NN\")).alias(\"v_min\")) #NN and min_id\n",
    "    NN_H_u = NN_H_min.select(array_except(col(\"NN\"), array(col(\"v_min\"))).alias(\"NN_u\"), \"v_min\") #NN-min_id, min_id\n",
    "    addEdge = NN_H_u.select(explode(NN_H_u.NN_u).alias(\"Node\"), \"v_min\") #New edges\n",
    "    addEdge_inv = addEdge.select(col(\"v_min\").alias(\"Node\"), col(\"Node\").alias(\"v_min\")) #Inverse direction of edges\n",
    "    allEdges = addEdge.union(addEdge_inv) #All edges that need to be in the new graph\n",
    "\n",
    "    G = allEdges.groupBy(\"Node\").agg(collect_set(\"v_min\").alias(\"NN\"))\n",
    "\n",
    "    #---------------Tree construction--------------\n",
    "    #The deactivated Nodes do not appear in G_{t+1}\n",
    "    deactiveNodes = dfH.select(\"Node\", array_min(col(\"NN\")).alias(\"v_min\"), \\\n",
    "                               isInside_udf(col(\"Node\"), col(\"NN\")).alias(\"Active\")).filter(col(\"Active\") == False)\n",
    "    #Tree in (Parent, Child) format; the one used in RDD\n",
    "    addEdge = deactiveNodes.select(col(\"v_min\").alias(\"Parent\"), col(\"Node\").alias(\"Child\"))\n",
    "    T = T.union(addEdge)\n",
    "\n",
    "    #--------------Find Seed-----------------\n",
    "    #Without broadcasting\n",
    "    #NN_H_G = dfH.join(G, dfH.Node == G.Node, how=\"left\").select(dfH.Node, dfH.NN.alias(\"H_NN\"), G.NN.alias(\"G_NN\"))\n",
    "    #With broadcasting\n",
    "    NN_H_G = dfH.join(broadcast(G), dfH.Node == G.Node, how=\"left\").select(dfH.Node, dfH.NN.alias(\"H_NN\"), G.NN.alias(\"G_NN\"))\n",
    "    joined_NN = NN_H_G.select(\"Node\", joinList_udf(col(\"H_NN\"), col(\"G_NN\")).alias(\"NN\"))\n",
    "    seed = joined_NN.filter( (size(col(\"NN\"))<= 1) & ( isInside_udf(col(\"Node\"), col(\"NN\")) ) )\n",
    "    Seeds = Seeds.union(seed.select(\"Node\"))\n",
    "\n",
    "    return G, T, Seeds\n",
    "\n",
    "#---------------Cracker main functions------------------\n",
    "def cracker(G):\n",
    "    n = 0\n",
    "    empty = sc.parallelize([])\n",
    "    T = sqlContext.createDataFrame(empty, schemaT)\n",
    "    Seeds = sqlContext.createDataFrame(empty, schemaS)\n",
    "\n",
    "    while G.count() != 0:\n",
    "        n += 1\n",
    "        H = Min_Selection_Step(G)\n",
    "        G, T, Seeds = Pruning_Step(H, T, Seeds)\n",
    "\n",
    "    return Seeds.count()\n",
    "\n",
    "#-------------Seed Propagation: broadcasting----------\n",
    "def seedPropagation(Tree, Seeds):\n",
    "    n = 0\n",
    "    T_seed = Tree.join(Seeds, Tree.Parent == Seeds.Node, how= \"left\").select(col(\"Child\").alias(\"Node\"), col(\"Node\").alias(\"Seed\"))\n",
    "    Seeds = Seeds.select(Seeds.Node, Seeds.Node)\n",
    "    while T_seed.filter(T_seed.Seed.isNull()).count() != 0:\n",
    "        n += 1\n",
    "        Seeds = T_seed.filter(T_seed.Seed.isNotNull()).union(Seeds)\n",
    "        T_seed = Tree.join(Seeds, Tree.Parent == Seeds.Node, how= \"left\").select(col(\"Child\").alias(\"Node\"), col(\"Seed\"))\n",
    "\n",
    "    return T_seed\n",
    "\n",
    "#------------Running the code-----------------------\n",
    "init = time.time()\n",
    "\n",
    "#data_raw = sc.textFile(\"hdfs:///user/hadoop/wc/input/GRAF_2MB_int.txt\")\n",
    "#G = data_raw.map(lambda x: x.split(',')).map(lambda x: (int(x[0]), int(x[1]))).flatMap(lambda x: [x, (x[1], x[0])]).groupByKey().mapValues(lambda x: list(set(x)))\n",
    "\n",
    "#debug dataset\n",
    "data_raw = sc.parallelize([(0,1), (1,2), (2,5), (5,8), (7,8), (3,7), (3,4), (3,6), (10,11), (10,12), (12,13), (6,9), (9,15), (9,16)])\n",
    "G = data_raw.flatMap(lambda x: [x, (x[1], x[0])]).groupByKey().mapValues(lambda x: list(set(x)) )\n",
    "\n",
    "dfG = sqlContext.createDataFrame(G, schemaG)\n",
    "dfG.printSchema()\n",
    "dfG.count()\n",
    "\n",
    "\n",
    "#Cracker with findSeeds\n",
    "Tree, Seeds = cracker(dfG)\n",
    "print(Seeds.show())\n",
    "seed_time = time.time()\n",
    "#do we need persist here?\n",
    "prop_time = time.time()\n",
    "prop = Seed_Propagation_lite(Tree, Seeds)\n",
    "prop.show()\n",
    "\n",
    "end = time.time()\n",
    "print(\"Time employed: %f\" % (seed_time - init))\n",
    "print(\"Time prop: %f\" % (end - prop_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
