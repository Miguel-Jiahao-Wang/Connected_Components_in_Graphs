{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "conf = SparkConf().setAppName(\"pyspark\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I think it is not possible to use sets inside dataframes, so I had to use ArrayType\n",
    "btc_raw = sc.parallelize([(0,1), (1,2), (2,5), (5,8), (7,8), (3,7), (3,4), (3,6), (10,11), (10,12), (12,13)])\n",
    "G = btc_raw.flatMap(lambda x: [x, (x[1], x[0])]).groupByKey().map(lambda x: ( [x[0]], list(set(x[1])) ))\n",
    "G_old = btc_raw.flatMap(lambda x: [x, (x[1], x[0])]).groupByKey().map(lambda x: ( x[0], set(x[1]) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G_new: [([0], [1]), ([8], [5, 7]), ([4], [3]), ([12], [10, 13]), ([1], [0, 2]), ([5], [8, 2]), ([13], [12]), ([2], [1, 5]), ([6], [3]), ([10], [11, 12]), ([7], [8, 3]), ([3], [4, 6, 7]), ([11], [10])]\n",
      "\n",
      "G_old: [(0, {1}), (8, {5, 7}), (4, {3}), (12, {10, 13}), (1, {0, 2}), (5, {8, 2}), (13, {12}), (2, {1, 5}), (6, {3}), (10, {11, 12}), (7, {8, 3}), (3, {4, 6, 7}), (11, {10})]\n"
     ]
    }
   ],
   "source": [
    "print(\"G_new:\", G.collect())\n",
    "print(\"\\nG_old:\", G_old.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "schemaList = [\"Node\", \"NN\"]\n",
    "schemaType = [ArrayType( IntegerType() ), ArrayType( IntegerType() )]\n",
    "schemaNull = [False, True]\n",
    "\n",
    "fields = [StructField(schemaList[0], schemaType[0], schemaNull[0]),\\\n",
    "          StructField(schemaList[1], schemaType[1], schemaNull[1])]\n",
    "\n",
    "schema = StructType(fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying schema to RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Node: array (nullable = false)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- NN: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfG = sqlContext.createDataFrame(G, schema)\n",
    "dfG.createOrReplaceTempView(\"graph\")\n",
    "dfG.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "|Node|       NN|\n",
      "+----+---------+\n",
      "| [0]|      [1]|\n",
      "| [8]|   [5, 7]|\n",
      "| [4]|      [3]|\n",
      "|[12]| [10, 13]|\n",
      "| [1]|   [0, 2]|\n",
      "| [5]|   [8, 2]|\n",
      "|[13]|     [12]|\n",
      "| [2]|   [1, 5]|\n",
      "| [6]|      [3]|\n",
      "|[10]| [11, 12]|\n",
      "| [7]|   [8, 3]|\n",
      "| [3]|[4, 6, 7]|\n",
      "|[11]|     [10]|\n",
      "+----+---------+\n",
      "\n",
      "+----+\n",
      "|Node|\n",
      "+----+\n",
      "| [0]|\n",
      "| [8]|\n",
      "| [4]|\n",
      "|[12]|\n",
      "| [1]|\n",
      "| [5]|\n",
      "|[13]|\n",
      "| [2]|\n",
      "| [6]|\n",
      "|[10]|\n",
      "| [7]|\n",
      "| [3]|\n",
      "|[11]|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"SELECT * FROM graph\").show()\n",
    "dfG.select(\"Node\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Min_Selection_Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|array(Node)|collect_set(v_min)|\n",
      "+-----------+------------------+\n",
      "|       [12]|          [12, 10]|\n",
      "|        [1]|            [0, 1]|\n",
      "|       [13]|              [10]|\n",
      "|        [6]|               [3]|\n",
      "|        [3]|               [3]|\n",
      "|        [5]|            [1, 5]|\n",
      "|        [4]|               [3]|\n",
      "|        [8]|            [2, 3]|\n",
      "|        [7]|            [5, 3]|\n",
      "|       [10]|              [10]|\n",
      "|       [11]|              [10]|\n",
      "|        [2]|            [0, 2]|\n",
      "|        [0]|               [0]|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Min_Selection_Step in pieces\n",
    "new_nodes = dfG.select( \"NN\", array_min( array_union(col(\"Node\"), col(\"NN\"))  ).alias(\"v_min\"))\n",
    "new_nodes = new_nodes.select(explode(new_nodes.NN).alias(\"Node\"), \"v_min\")\n",
    "new_nodes = new_nodes.groupBy(array(\"Node\")).agg(collect_set(\"v_min\"))\n",
    "new_nodes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Min_Selection_Step(df_G):\n",
    "    v_min = df_G.select( \"NN\", array_min( array_union(col(\"Node\"), col(\"NN\"))  ).alias(\"v_min\"))\n",
    "    addEdge = v_min.select(explode(v_min.NN).alias(\"Node\"), \"v_min\")\n",
    "    dfH = addEdge.groupBy(array(\"Node\").alias(\"Node\")).agg(collect_set(\"v_min\").alias(\"v_min\"))\n",
    "    return dfH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|array(Node)|collect_set(v_min)|\n",
      "+-----------+------------------+\n",
      "|       [12]|          [12, 10]|\n",
      "|        [1]|            [0, 1]|\n",
      "|       [13]|              [10]|\n",
      "|        [6]|               [3]|\n",
      "|        [3]|               [3]|\n",
      "|        [5]|            [1, 5]|\n",
      "|        [4]|               [3]|\n",
      "|        [8]|            [2, 3]|\n",
      "|        [7]|            [5, 3]|\n",
      "|       [10]|              [10]|\n",
      "|       [11]|              [10]|\n",
      "|        [2]|            [0, 2]|\n",
      "|        [0]|               [0]|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfH = Min_Selection_Step(dfG)\n",
    "dfH.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning_Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_min = dfH.select(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pruning_Step(H, T, Seeds):\n",
    "    #H = H.cache()\n",
    "    #minimum node of the neighborhood: shared for following parts\n",
    "    v_min = H.mapValues(lambda x: min(x))\n",
    "    v_min_bc = sc.broadcast(dict(v_min.collect())) #Broadcasting v_min\n",
    "    \n",
    "    #---------------G construction-------------------\n",
    "    H_filtered = H.filter(lambda x: len(x[1]) > 1)\n",
    "    NN_H_u = H_filtered.mapValues(lambda x: x - {min(x)} )\n",
    "    #With Broadcasting\n",
    "    addEdge2=NN_H_u.map(lambda x:(x[0],(x[1],v_min_bc.value[x[0]]))).flatMap(lambda x:[(x[1][1],y) for y in x[1][0]])\n",
    "    #Without broadcasting\n",
    "    #addEdge2 = NN_H_u.join(v_min).flatMap(lambda x: [(x[1][1], y) for y in x[1][0]])\n",
    "    G = addEdge2.flatMap(lambda x: [x, (x[1], x[0])]).groupByKey().mapValues(lambda x: set(x))\n",
    "    \n",
    "    #---------------Tree construction--------------\n",
    "    #The deactivated Nodes do not appear in G_{t+1}\n",
    "    deactiveNodes = H.filter(lambda x: x[0] not in x[1]).mapValues(lambda x: False)\n",
    "    #Without broadcasting\n",
    "    #addEdge3 = deactiveNodes.join(v_min).map(lambda x: (x[1][1], x[0]))\n",
    "    #With Broadcasting\n",
    "    addEdge3 = deactiveNodes.map(lambda x: (x[0], (x[1], v_min_bc.value[x[0]]))).map(lambda x: (x[1][1], x[0]))\n",
    "    T = T.union(addEdge3)\n",
    "\n",
    "    #--------------Find Seed-----------------\n",
    "    #Elements in H with neighborhood from G_{t+1}\n",
    "    NN_G_H = H.cogroup(G).mapValues(lambda x: (list(x[0]), list(x[1])) ).mapValues(lambda x: set_join(x) )\n",
    "\n",
    "    #Not sure is necessary to use True/False\n",
    "    #deactivated = NN_G_H.cogroup(deactiveNodes).map(lambda x: (x[0], (list(x[1][0]), list(x[1][1])) ))\n",
    "    #seed = deactivated.filter(lambda x: (len(x[1][0]) <= 1) & (x[0] in x[1][0]) & x[1][1]) \n",
    "    \n",
    "    seed = NN_G_H.filter(lambda x: (len(x[1]) <= 1) & (x[0] in x[1]))\n",
    "    Seeds = Seeds.union(seed)\n",
    "\n",
    "    return [G, T, Seeds]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
