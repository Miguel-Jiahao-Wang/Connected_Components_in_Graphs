{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "conf = SparkConf().setAppName(\"pyspark\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I think it is not possible to use sets inside dataframes, so I had to use ArrayType\n",
    "btc_raw = sc.parallelize([(0,1), (1,2), (2,5), (5,8), (7,8), (3,7), (3,4), (3,6), (10,11), (10,12), (12,13)])\n",
    "G = btc_raw.flatMap(lambda x: [x, (x[1], x[0])]).groupByKey().map(lambda x: ( x[0], list(set(x[1])) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graph\n",
    "schemaList = [\"Node\", \"NN\"]\n",
    "schemaType = [ IntegerType() , ArrayType( IntegerType() )]\n",
    "schemaNull = [False, True]\n",
    "\n",
    "fields = [StructField(schemaList[0], schemaType[0], schemaNull[0]),\\\n",
    "          StructField(schemaList[1], schemaType[1], schemaNull[1])]\n",
    "\n",
    "schemaG = StructType(fields)\n",
    "\n",
    "#Tree with (Parent, Child) format\n",
    "schemaList2 = [\"Parent\", \"Child\"]\n",
    "schemaType2 = [ IntegerType() , IntegerType() ]\n",
    "schemaNull2 = [False, True]\n",
    "\n",
    "fields2 = [StructField(schemaList2[0], schemaType2[0], schemaNull2[0]),\\\n",
    "          StructField(schemaList2[1], schemaType2[1], schemaNull2[1])]\n",
    "\n",
    "schemaT = StructType(fields2)\n",
    "\n",
    "#Tree with (Parent, Children) format\n",
    "schemaList21 = [\"Parent\", \"Children\"]\n",
    "schemaType21 = [ IntegerType(), ArrayType( IntegerType() )]\n",
    "schemaNull21 = [False, True]\n",
    "\n",
    "fields21 = [StructField(schemaList21[0], schemaType21[0], schemaNull21[0]),\\\n",
    "          StructField(schemaList21[1], schemaType21[1], schemaNull21[1])]\n",
    "\n",
    "schemaT1 = StructType(fields21)\n",
    "\n",
    "#Seeds\n",
    "schemaList3 = [\"Node\"]\n",
    "schemaType3 = [IntegerType()]\n",
    "schemaNull3 = [True]\n",
    "\n",
    "fields3 = [StructField(schemaList3[0], schemaType3[0], schemaNull3[0])]\n",
    "\n",
    "schemaS = StructType(fields3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying schema to RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Node: integer (nullable = false)\n",
      " |-- NN: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfG = sqlContext.createDataFrame(G, schemaG)\n",
    "dfG.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Min_Selection_Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nodes as int\n",
    "def Min_Selection_Step(df_G):\n",
    "    #Including node inside NN_H and finding the min_id inside the NN_H\n",
    "    NN_min = df_G.select(array_union(col(\"NN\"), array(col(\"Node\")).alias(\"Node\")).alias(\"NN\"), \\\n",
    "                         array_min( array_union(array(col(\"Node\")).alias(\"Node\"), col(\"NN\"))  ).alias(\"v_min\"))\n",
    "    #All edges that needs to be added to G_{t+1}\n",
    "    addEdge = NN_min.select(explode(NN_min.NN).alias(\"Node\"), \"v_min\")\n",
    "    #Grouping all node_id\n",
    "    dfH = addEdge.groupBy(\"Node\").agg(collect_set(\"v_min\").alias(\"NN\"))\n",
    "    return dfH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "|Node|       NN|\n",
      "+----+---------+\n",
      "|  12| [12, 10]|\n",
      "|   1|   [0, 1]|\n",
      "|  13| [12, 10]|\n",
      "|   6|      [3]|\n",
      "|   3|      [3]|\n",
      "|   5|[1, 5, 2]|\n",
      "|   4|      [3]|\n",
      "|   8|[5, 2, 3]|\n",
      "|   7|   [5, 3]|\n",
      "|  10|     [10]|\n",
      "|  11|     [10]|\n",
      "|   2|[0, 1, 2]|\n",
      "|   0|      [0]|\n",
      "+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfH = Min_Selection_Step(dfG)\n",
    "dfH.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning_Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to check if int is inside an array \n",
    "#Used in deactiveNodes\n",
    "def isInside(node, NN):\n",
    "    if node in NN:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "isInside_udf = udf(isInside, BooleanType())\n",
    "\n",
    "#Function to join two list \n",
    "#Used to find seeds\n",
    "def joinList(H_NN, G_NN):\n",
    "    #I need to think if H_NN can be empty\n",
    "    if G_NN != None:\n",
    "        return list(set(G_NN + H_NN))\n",
    "    else:\n",
    "        return H_NN\n",
    "    \n",
    "joinList_udf = udf(joinList, ArrayType(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pruning_Step(dfH, T, Seeds):\n",
    "    #---------------G construction-------------------\n",
    "    H_filtered = dfH.filter(size(col(\"NN\")) > 1) #NN with more than 1 element\n",
    "    NN_H_min = H_filtered.select(\"NN\", array_min(col(\"NN\")).alias(\"v_min\")) #NN and min_id\n",
    "    NN_H_u = NN_H_min.select(array_except(col(\"NN\"), array(col(\"v_min\"))).alias(\"NN_u\"), \"v_min\") #NN-min_id, min_id\n",
    "\n",
    "    addEdge = NN_H_u.select(explode(NN_H_u.NN_u).alias(\"Node\"), \"v_min\") #New edges\n",
    "    addEdge_inv = addEdge.select(col(\"v_min\").alias(\"Node\"), col(\"Node\").alias(\"v_min\")) #Inverse direction of edges\n",
    "    allEdges = addEdge.union(addEdge_inv) #All edges that need to be in the new graph\n",
    "\n",
    "    G = allEdges.groupBy(\"Node\").agg(collect_set(\"v_min\").alias(\"NN\"))\n",
    "    \n",
    "    #---------------Tree construction--------------\n",
    "    #The deactivated Nodes do not appear in G_{t+1}\n",
    "    deactiveNodes = dfH.select(\"Node\", array_min(col(\"NN\")).alias(\"v_min\"), \\\n",
    "                               isInside_udf(col(\"Node\"), col(\"NN\")).alias(\"Active\")).filter(col(\"Active\") == False)\n",
    "    \n",
    "    #Tree in (Parent, Child) format; the one used in RDD\n",
    "    addEdge = deactiveNodes.select(col(\"v_min\").alias(\"Parent\"), col(\"Node\").alias(\"Child\"))\n",
    "    T = T.union(addEdge)\n",
    "    \n",
    "    #Tree in (Parent, Children) format; Graph like format\n",
    "    #addEdge = deactiveNodes.groupBy(col(\"v_min\").alias(\"Parent\")).agg(collect_set(\"Node\").alias(\"Children\"))\n",
    "    #T = T.union(addEdge)\n",
    "    \n",
    "    \n",
    "    #--------------Find Seed-----------------\n",
    "    NN_H_G = dfH.join(broadcast(G), dfH.Node == G.Node, how=\"left\").select(dfH.Node, dfH.NN.alias(\"H_NN\"), G.NN.alias(\"G_NN\"))\n",
    "    joined_NN = NN_H_G.select(\"Node\", joinList_udf(col(\"H_NN\"), col(\"G_NN\")).alias(\"NN\"))\n",
    "    seed = joined_NN.filter( (size(col(\"NN\"))<= 1) & ( isInside_udf(col(\"Node\"), col(\"NN\")) ) ) \n",
    "    Seeds = Seeds.union(seed.select(\"Node\"))\n",
    "    \n",
    "    return G, T, Seeds\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cracker: Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cracker(G):\n",
    "    n = 0\n",
    "    empty = sc.parallelize([])\n",
    "    T = sqlContext.createDataFrame(empty, schemaT)\n",
    "    Seeds = sqlContext.createDataFrame(empty, schemaS)\n",
    "\n",
    "    while G.count() != 0:\n",
    "        n += 1\n",
    "        H = Min_Selection_Step(G)\n",
    "        G, T, Seeds = Pruning_Step(H, T, Seeds)\n",
    "    \n",
    "    #Building a graph format (Parent, Children) from (Parent, Child): OPTIONAL\n",
    "    Tree = T.groupBy(col(\"Parent\")).agg(collect_set(\"Child\").alias(\"Children\"))\n",
    "    \n",
    "    return Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+\n",
      "|Parent|    Children|\n",
      "+------+------------+\n",
      "|     3|   [6, 7, 4]|\n",
      "|    10|[12, 13, 11]|\n",
      "|     2|         [8]|\n",
      "|     0|[1, 5, 2, 3]|\n",
      "+------+------------+\n",
      "\n",
      "CPU times: user 774 ms, sys: 289 ms, total: 1.06 s\n",
      "Wall time: 30.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Tree = cracker(dfG)\n",
    "Tree.show()\n",
    "# Can this function output number of CC?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other possibilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nodes as lists\n",
    "\n",
    "G = btc_raw.flatMap(lambda x: [x, (x[1], x[0])]).groupByKey().map(lambda x: ( [x[0]], list(set(x[1])) ))\n",
    "\n",
    "\n",
    "schemaList = [\"Node\", \"NN\"]\n",
    "schemaType = [ArrayType( IntegerType() ), ArrayType( IntegerType() )]\n",
    "schemaNull = [False, True]\n",
    "\n",
    "fields = [StructField(schemaList[0], schemaType[0], schemaNull[0]),\\\n",
    "          StructField(schemaList[1], schemaType[1], schemaNull[1])]\n",
    "\n",
    "schema = StructType(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nodes as lists\n",
    "def Min_Selection_Step(df_G):\n",
    "    #Including node inside NN_H and finding the min_id inside the NN_H\n",
    "    NN_min = df_G.select(array_union(col(\"NN\"), col(\"Node\")).alias(\"NN\"), \\\n",
    "                         array_min( array_union(col(\"Node\"), col(\"NN\"))  ).alias(\"v_min\"))\n",
    "    #All edges that needs to be added to G_{t+1}\n",
    "    addEdge = NN_min.select(explode(NN_min.NN).alias(\"Node\"), \"v_min\")\n",
    "    #Grouping all node_id\n",
    "    dfH = addEdge.groupBy(array(\"Node\").alias(\"Node\")).agg(collect_set(\"v_min\").alias(\"NN\"))\n",
    "    return dfH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tree in Edge = (Parent, Child) format\n",
    "schemaList2 = [\"Parent\", \"Child\"]\n",
    "schemaType2 = [ IntegerType() , IntegerType()]\n",
    "schemaNull2 = [False, True]\n",
    "\n",
    "fields2 = [StructField(schemaList2[0], schemaType2[0], schemaNull2[0]),\\\n",
    "          StructField(schemaList2[1], schemaType2[1], schemaNull2[1])]\n",
    "\n",
    "schemaT = StructType(fields2)\n",
    "\n",
    "\n",
    "empty = sc.parallelize([])\n",
    "T = sqlContext.createDataFrame(empty, schemaT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
