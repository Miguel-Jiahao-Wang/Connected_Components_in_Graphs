{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "conf = SparkConf().setAppName(\"pyspark\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I think it is not possible to use sets inside dataframes, so I had to use ArrayType\n",
    "btc_raw = sc.parallelize([(0,1), (1,2), (2,5), (5,8), (7,8), (3,7), (3,4), (3,6), (10,11), (10,12), (12,13)])\n",
    "G = btc_raw.flatMap(lambda x: [x, (x[1], x[0])]).groupByKey().map(lambda x: ( [x[0]], list(set(x[1])) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([0], [1]),\n",
       " ([8], [5, 7]),\n",
       " ([4], [3]),\n",
       " ([12], [10, 13]),\n",
       " ([1], [0, 2]),\n",
       " ([5], [8, 2]),\n",
       " ([13], [12]),\n",
       " ([2], [1, 5]),\n",
       " ([6], [3]),\n",
       " ([10], [11, 12]),\n",
       " ([7], [8, 3]),\n",
       " ([3], [4, 6, 7]),\n",
       " ([11], [10])]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "schemaList = [\"Node\", \"NN\"]\n",
    "schemaType = [ArrayType( IntegerType() ), ArrayType( IntegerType() )]\n",
    "schemaNull = [False, True]\n",
    "\n",
    "fields = [StructField(schemaList[0], schemaType[0], schemaNull[0]),\\\n",
    "          StructField(schemaList[1], schemaType[1], schemaNull[1])]\n",
    "\n",
    "schema = StructType(fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying schema to RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Node: array (nullable = false)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- NN: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfG = sqlContext.createDataFrame(G, schema)\n",
    "dfG.createOrReplaceTempView(\"graph\")\n",
    "dfG.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "|Node|       NN|\n",
      "+----+---------+\n",
      "| [0]|      [1]|\n",
      "| [8]|   [5, 7]|\n",
      "| [4]|      [3]|\n",
      "|[12]| [10, 13]|\n",
      "| [1]|   [0, 2]|\n",
      "| [5]|   [8, 2]|\n",
      "|[13]|     [12]|\n",
      "| [2]|   [1, 5]|\n",
      "| [6]|      [3]|\n",
      "|[10]| [11, 12]|\n",
      "| [7]|   [8, 3]|\n",
      "| [3]|[4, 6, 7]|\n",
      "|[11]|     [10]|\n",
      "+----+---------+\n",
      "\n",
      "+----+\n",
      "|Node|\n",
      "+----+\n",
      "| [0]|\n",
      "| [8]|\n",
      "| [4]|\n",
      "|[12]|\n",
      "| [1]|\n",
      "| [5]|\n",
      "|[13]|\n",
      "| [2]|\n",
      "| [6]|\n",
      "|[10]|\n",
      "| [7]|\n",
      "| [3]|\n",
      "|[11]|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"SELECT * FROM graph\").show()\n",
    "dfG.select(\"Node\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|Node|v_min|\n",
      "+----+-----+\n",
      "| [0]|  [0]|\n",
      "| [8]|  [5]|\n",
      "| [4]|  [3]|\n",
      "|[12]| [10]|\n",
      "| [1]|  [0]|\n",
      "| [5]|  [2]|\n",
      "|[13]| [12]|\n",
      "| [2]|  [1]|\n",
      "| [6]|  [3]|\n",
      "|[10]| [10]|\n",
      "| [7]|  [3]|\n",
      "| [3]|  [3]|\n",
      "|[11]| [10]|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "v_min = dfG.select( \"Node\", array(array_min( array_union(col(\"Node\"), col(\"NN\")) ) ).alias(\"v_min\"))\n",
    "                   \n",
    "v_min.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Min_Selection_Step(G): #dictionary format RDD\n",
    "    v_min = G.map(lambda x: (x[0], min(x[1] | {x[0]})))  #See sql.functions.array_intersect()\n",
    "    NN_G_u = G.map(lambda x: (x[0], x[1] | {x[0]}))\n",
    "    #Broadcasting\n",
    "    v_min_bc = sc.broadcast(dict(v_min.collect()))\n",
    "    addEdge = NN_G_u.map(lambda x: (x[0], (x[1], v_min_bc.value[x[0]])) )\n",
    "    addEdge1 = addEdge.flatMap(lambda x: [(y, x[1][1]) for y in x[1][0]])\n",
    "    #Without broadcasting\n",
    "    #addEdge1 = NN_G_u.join(v_min).flatMap(lambda x: [(y, x[1][1]) for y in x[1][0]])\n",
    "\n",
    "    H = addEdge1.groupByKey().mapValues(lambda x: set(x))\n",
    "    return H"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
